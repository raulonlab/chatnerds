system_prompt: >
  You are a helpful assistant, you will use the provided context to answer user questions.
  Read the given context before answering questions and think step by step. If you can not answer a user question based on 
  the provided context, inform the user. Do not use any other information for answering user. Provide a detailed answer to the question.
use_history: true
device_type: mps    # (default: cpu) Select device type: cuda, mps, cpu
prompt_type: mistral
summarize_chain_type: "map_reduce"  # (default: stuff) Select summarization chain type: stuff, map_reduce, refine

embeddings:
  model_name: hkunlp/instructor-large
  # model_kwargs:
  #   device: mps

llm: mistral_gguf

# https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF
mistral_gguf:
  model_id: TheBloke/Mistral-7B-Instruct-v0.2-GGUF
  model_basename: mistral-7b-instruct-v0.2.Q4_K_M.gguf

  # https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/llms/llamacpp.py
  # llamacpp kwargs
  temperature: 0.5  # Default: 0.8
  top_p: 0.5  # Default: 0.95
  n_ctx: 2048
  max_tokens: 4096
  n_batch: 512  # set this based on your GPU & CPU RAM
  n_gpu_layers: 32

speechless_wizard_gguf:
  model_id: TheBloke/Speechless-Llama2-Hermes-Orca-Platypus-WizardLM-13B-GGUF
  model_basename: speechless-llama2-hermes-orca-platypus-wizardlm-13b.Q4_K_M.gguf

  # llamacpp kwargs
  temperature: 0.2
  top_p: 0.95
  n_ctx: 4096
  max_tokens: 4096
  n_batch: 512  # set this based on your GPU & CPU RAM
  n_gpu_layers: 32

wizard_gguf:
  model_id: TheBloke/WizardLM-7B-uncensored-GGUF
  model_basename: WizardLM-7B-uncensored.Q4_K_M.gguf
  # https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md
  # llamacpp kwargs
  temperature: 0.5  # Default: 0.8
  top_p: 0.5  # Default: 0.95
  n_ctx: 2048
  max_tokens: 4096
  n_batch: 512  # set this based on your GPU & CPU RAM
  n_gpu_layers: 32

# https://artificialcorner.com/run-mistral7b-quantized-for-free-on-any-computer-2cadc18b45a2
# https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ#how-to-use-this-gptq-model-from-python-code
mistral_gptq:
  model_id: TheBloke/Mistral-7B-Instruct-v0.1-GPTQ
  model_basename: model.safetensors
  # temperature: 0.2
  # n_ctx: 4096
  # max_tokens: 4096
  # top_p: 0.95
  # n_batch: 512  # set this based on your GPU & CPU RAM
  # n_gpu_layers: 32

wizard_gptq:
  model_id: TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ
  model_basename: model.safetensors 

huggingface_hf:
  model_id: TheBloke/Wizard-Vicuna-7B-Uncensored-HF
  model_basename: WizardLM-7B-uncensored.Q4_K_M.gguf
  model_class: HuggingFace
  model_kwargs:
    temperature: 0.2
    n_ctx: 4096
    max_tokens: 4096
    top_p: 0.95
    n_batch: 512  # set this based on your GPU & CPU RAM
    n_gpu_layers: 32
    local_files_only: false
  pipeline_kwargs:
    max_new_tokens: 4096

chroma:
  is_persistent: true
  anonymized_telemetry: false

retriever:
  search_kwargs:
    k: 4
