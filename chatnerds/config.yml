use_history: false  # Adding history to conversations is WIP
device_type: cpu    # (default: cpu) Select device type: cuda, mps, cpu

# Default model
# Select the key of one of the available presets in 'config.models.yml'
default_model: mistral-7b-instruct-v0.1-gguf

# Select the key of one of the vector store provider (chroma or qdrant)
vector_store: chroma

# HuggingFace embeddings (using sentence-transformer). Ex. model_name: hkunlp/instructor-large, BAAI/bge-large-en-v1.5
embeddings:
  model_name: hkunlp/instructor-large
  # encode_kwargs:
  #   normalize_embeddings: false  # (default: false) Normalize embeddings before storing them in the index.
  # model_kwargs:
  #   device: mps  # mps is not available yet in HuggingFace embeddings

splitter:
  chunk_size: 1000  # (default: 1000) Maximum number of tokens per chunk or model max_seq_length if larger
  chunk_overlap: 0  # (default: 0) Number of tokens to overlap between chunks.
  # keep_separator: false  # (default: false) Keep the separator token at the end of each chunk.

retriever:
  search_type: similarity  # Defines the type of search that the Retriever should perform: "similarity" (default), "mmr", or "similarity_score_threshold".
  search_kwargs:
    k: 20  # Max number of documents to retrieve when searching for similar documents (Default: 20)
    # score_threshold: 0.85  # Minimum relevance threshold for similarity_score_threshold
    # fetch_k: 500  # Amount of documents to pass to MMR algorithm (Default: 20)
    # lambda_mult: 0.2  # Diversity of results returned by MMR; 1 for minimum diversity and 0 for maximum. (Default: 0.5)
    # filter: Filter by document metadata. Ex: {'paper_title':'GPT-4 Technical Report'}}

reranker:
  model_name: cross-encoder/ms-marco-MiniLM-L-6-v2  # BAAI/bge-reranker-large
  # num_labels:
  # max_length:
  # device:
  # tokenizer_args:
  # automodel_args:

summarize:
  # model: mistral-7b-instruct-v0.1-gguf
  summarize_chain_type: "map_reduce"  # (default: map_reduce) Select summarization chain type: stuff, map_reduce, refine

chat_chain:
  n_expanded_questions: 3  # Number of similar questions to expand the original query with. Set 0 to disable query expansion. (Default: 3)
  use_cross_encoding_rerank: true  # Use cross-encoding reranking of retrieved documents. (Default: true)
  n_combined_documents: 6  # Number of documents to combine as a context for the prompt sent to the LLM. (Default: 6)

retrieve_chain: chat_chain

chroma:
  is_persistent: true
  anonymized_telemetry: false

qdrant:
  # location: ":memory:"  # Local mode with in-memory storage only
  # path: "/tmp/local_qdrant"
  # url: "http://localhost:6333/..."
  # prefer_grpc: true
