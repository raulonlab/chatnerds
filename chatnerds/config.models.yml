# Initial model presets for nerds

openai-gpt-3.5-turbo:
  provider: openai
  model_name: gpt-3.5-turbo
  temperature: 0.2
  # top_p: 0.95
  # max_tokens: 4096
  # verbose: false

openai-gpt-4:
  provider: openai
  model_name: gpt-4
  temperature: 0.2
  # top_p: 0.95
  # max_tokens: 8192
  # verbose: false

ollama-llama2-chat:
  provider: ollama
  model: llama2-uncensored:7b-chat-q4_K_M
  temperature: 0.5  # Default: 0.8
  # top_p: 0.5  # Default: 0.9
  num_ctx: 16000  # 8192  # Default: 2048
  # verbose: false

ollama-mistral-instruct:
  provider: ollama
  model: mistral:instruct
  temperature: 0.2  # Default: 0.8
  # top_p: 0.5  # Default: 0.9
  num_ctx: 8192  # 8192  # Default: 2048
  # verbose: false

ollama-qwen-7b-chat: # https://ollama.com/library/qwen:7b-chat
  provider: ollama
  model: qwen:7b-chat
  temperature: 0.2  # Default: 0.8
  # top_p: 0.5  # Default: 0.9
  num_ctx: 32768
  # verbose: false

ollama-openchat:
  provider: ollama
  model: openchat:7b
  temperature: 0.2  # Default: 0.8
  # top_p: 0.5  # Default: 0.9
  num_ctx: 8192  # 8192  # Default: 2048
  # verbose: false

ollama-gemma-instruct:
  provider: ollama
  model: gemma:7b-instruct
  temperature: 0.2  # Default: 0.8
  # top_p: 0.5  # Default: 0.9
  num_ctx: 8192  # 8192  # Default: 2048
  # verbose: false

lmstudio-localhost:
  provider: openai
  model_name: local_model
  base_url: http://localhost:1234/v1
  temperature: 0.2
  # top_p: 0.95
  # max_tokens: 4096
  # verbose: false

mistral-7b-instruct-v0.1-gguf:
  provider: llamacpp
  prompt_type: mistral
  model_id: TheBloke/Mistral-7B-Instruct-v0.1-GGUF
  model_basename: mistral-7b-instruct-v0.1.Q4_K_M.gguf
  temperature: 0.2
  # top_p: 0.95
  n_ctx: 16000  # 16000
  # max_tokens: 8192  # 4096
  n_batch: 512  # 512 (Default: 8) set this based on your GPU & CPU RAM
  n_gpu_layers: -1

# https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF
mistral-7b-instruct-v0.2-gguf:
  provider: llamacpp
  prompt_type: mistral
  model_id: TheBloke/Mistral-7B-Instruct-v0.2-GGUF
  model_basename: mistral-7b-instruct-v0.2.Q4_K_M.gguf
  # https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/llms/llamacpp.py
  # llamacpp kwargs
  temperature: 0.2  # Default: 0.8
  # top_p: 0.5  # Default: 0.9
  n_ctx: 4096 # https://medium.com/@mne/run-mistral-7b-model-on-macbook-m1-pro-with-16gb-ram-using-llama-cpp-44134694b773
  max_tokens: 4096
  n_batch: 512  # Should be between 1 and n_ctx, set this based on your GPU & CPU RAM
  # n_threads: 7
  n_gpu_layers: -1 # 130
  # verbose: false
  # seed: 42

mistral-7b-openhermes-gguf:  # https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF
  provider: llamacpp
  # prompt_type: mistral
  model_id: TheBloke/OpenHermes-2.5-Mistral-7B-GGUF
  model_basename: openhermes-2.5-mistral-7b.Q4_K_M.gguf
  # https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/llms/llamacpp.py
  # llamacpp kwargs
  temperature: 0.2  # Default: 0.8
  # top_p: 0.5  # Default: 0.9
  n_ctx: 8192 # https://medium.com/@mne/run-mistral-7b-model-on-macbook-m1-pro-with-16gb-ram-using-llama-cpp-44134694b773
  max_tokens: 8192
  n_batch: 512  # Should be between 1 and n_ctx, set this based on your GPU & CPU RAM
  # n_threads: 7
  n_gpu_layers: -1 # 130
  # verbose: false
  # seed: 42

gemma-7b-gguf:
  provider: llamacpp
  # prompt_type: gemma
  model_id: google/gemma-7b-GGUF
  model_basename: gemma-7b.gguf
  # llamacpp kwargs
  temperature: 0.2  # Default: 0.8
  # top_p: 0.5  # Default: 0.9
  n_ctx: 8192
  max_tokens: 8192
  n_batch: 512  # Should be between 1 and n_ctx, set this based on your GPU & CPU RAM
  # n_threads: 7
  n_gpu_layers: -1 # 130
  # verbose: false
  
  # See https://huggingface.co/google/gemma-7b-it/discussions/38#65d7b14adb51f7c160769fa1
  repeat_penalty: 1.0
  penalize_nl: false
  # in_prefix: "<start_of_turn>user\n" 
  # in_suffix: "<end_of_turn>\n<start_of_turn>model\n"

llama-speechless-13b-gguf:
  prompt_type: llama
  model_id: TheBloke/Speechless-Llama2-Hermes-Orca-Platypus-WizardLM-13B-GGUF
  model_basename: speechless-llama2-hermes-orca-platypus-wizardlm-13b.Q3_K_M.gguf
  # llamacpp kwargs
  temperature: 0.2
  top_p: 0.95
  n_ctx: 4096
  max_tokens: 4096
  n_batch: 512  # set this based on your GPU & CPU RAM
  n_gpu_layers: 1

qwen-1.5-7b-chat:  # https://huggingface.co/Qwen/Qwen1.5-7B-Chat-GGUF
  model_id: Qwen/Qwen1.5-7B-Chat-GGUF
  model_basename: qwen1_5-7b-chat-q4_k_m.gguf
  # llamacpp kwargs
  temperature: 0.2
  top_p: 0.95
  n_ctx: 32768
  # max_tokens: 4096
  n_batch: 512  # set this based on your GPU & CPU RAM
  n_gpu_layers: -1

# https://artificialcorner.com/run-mistral7b-quantized-for-free-on-any-computer-2cadc18b45a2
# https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ#how-to-use-this-gptq-model-from-python-code
mistral-7b-instruct-v0.1-gptq:
  prompt_type: mistral
  model_id: TheBloke/Mistral-7B-Instruct-v0.1-GPTQ
  model_basename: model.safetensors
  # temperature: 0.2
  # n_ctx: 4096
  # max_tokens: 4096
  # top_p: 0.95
  # n_batch: 512  # set this based on your GPU & CPU RAM
  # n_gpu_layers: 32

llama-wizard-vicuna-7b-uncensored-gptq:
  prompt_type: llama
  model_id: TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ
  model_basename: model.safetensors 

llama-wizard-vicuna-7b-uncensored-hf:
  prompt_type: llama
  model_id: TheBloke/Wizard-Vicuna-7B-Uncensored-HF
  model_basename: WizardLM-7B-uncensored.Q4_K_M.gguf
  model_kwargs:
    temperature: 0.2
    n_ctx: 4096
    max_tokens: 4096
    top_p: 0.95
    n_batch: 512  # set this based on your GPU & CPU RAM
    n_gpu_layers: 32
    local_files_only: false
  pipeline_kwargs:
    max_new_tokens: 4096
